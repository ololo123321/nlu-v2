# @package _global_
defaults:
  - override /tokenizer: roberta
model:
  bert:
    params_updates:
      attention_probs_dropout_prob: 0.1  # in case of large model
dataset:
  is_bpe: true
training:
  batch_size: 8  # large
optimizer:
  init_lr: 1e-5  # large