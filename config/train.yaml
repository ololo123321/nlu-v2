defaults:
  - _self_
  - hydra: hydra_default  # TODO

model:
  __target__: ???
  pretrained_dir: ???
  checkpoint_path: ???
  birnn:
    use: false
  bert:
    test_mode: false
    dropout: 0.2
    scope: bert
    pad_token_id: ???
    cls_token_id: ???
    sep_token_id: ???
    params: ???
    params_updates:
      attention_probs_dropout_prob: 0.5  # default 0.1
      hidden_dropout_prob: 0.1  # default 0.1

tokenizer:
  _target_: bert.tokenization.FullTokenizer
  vocab_file: ${model.pretrained_dir}/vocab.txt
  do_lower_case: false

optimizer:
  init_lr: 2e-5
  warmup_proportion: 0.1

dataset:
  _target_: ???
  data: null
  mode: ???
  tokenizer: ???
  tokens_expression: null
  ignore_bad_examples: true
  max_chunk_length: 512
  window: 3
  stride: 1
  language: ru
  fix_sent_pointers: true

training:
  num_epochs: 100
  batch_size: 16
  max_epochs_wo_improvement: 10

validation:
  window: ${dataset.window}

inference:
  max_tokens_per_batch: 10000
  window: ${dataset.window}
  max_chunk_length: ${dataset.max_chunk_length}

train_data_dir: ???
valid_data_dir: ???
output_dir: ???
num_examples_train: null
num_examples_valid: null