{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.57.0)\n",
      "Collecting rusenttokenize\n",
      "  Downloading https://files.pythonhosted.org/packages/25/4c/a2f00be5def774a3df2e5387145f1cb54e324607ec4a7e23f573645946e7/rusenttokenize-0.0.5-py3-none-any.whl\n",
      "Collecting nltk\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip (1.4MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4MB 12.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting click (from nltk)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/3d/fa76db83bf75c4f8d338c2fd15c8d33fdd7ad23a9b5e57eb6c5de26b430e/click-7.1.2-py2.py3-none-any.whl (82kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 36.4MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from nltk) (1.0.1)\n",
      "Collecting regex (from nltk)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/ad/e0a6ea246c70fe549d8ef4a4632e66cccbdaab4830b04735f44144ed9308/regex-2020.11.13-cp36-cp36m-manylinux2010_x86_64.whl (666kB)\n",
      "\u001b[K     |████████████████████████████████| 675kB 50.6MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/ae/8c/3f/b1fe0ba04555b08b57ab52ab7f86023639a526d8bc8d384306\n",
      "Successfully built nltk\n",
      "Installing collected packages: rusenttokenize, click, regex, nltk\n",
      "Successfully installed click-7.1.2 nltk-3.5 regex-2020.11.13 rusenttokenize-0.0.5\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 21.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install tqdm rusenttokenize nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0513 07:50:57.183933 139799476565824 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0513 07:50:57.186815 139799476565824 deprecation_wrapper.py:119] From /tf/datadrive/datascientist/relation-extraction/src/model/base.py:69: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "tf = import_tensorflow()\n",
    "from bert.tokenization import FullTokenizer\n",
    "\n",
    "import sys; sys.path.insert(0, \"/tf/datadrive/datascientist/relation-extraction/\")\n",
    "from src.data.io import parse_collection, simplify, read_file_v3\n",
    "from src.data.check import check_tokens, check_arcs, check_split\n",
    "from src.data.preprocessing import split_example_v2, apply_bpe, enumerate_entities, fit_encodings, apply_encodings\n",
    "from src.data.base import NerEncodings, NerPrefixJoiners, Languages\n",
    "from src.model.base import ModeKeys\n",
    "from src.model.utils import get_session\n",
    "from src.model.coreference_resolution import BertForCoreferenceResolutionMentionRanking\n",
    "from src.utils import train_test_valid_split, classification_report_to_string\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "assert tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"/tf/datadrive/nn_lfs/experiments/bert_for_cr_mention_ranking_10_epochs_fixed\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Получение выборки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **загрузка**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4/182 [00:00<00:05, 30.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num .txt files: 182\n",
      "num .ann files: 182\n",
      "num annotated texts: 182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182/182 [00:02<00:00, 75.33it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully parsed 182 examples from 182 files.\n",
      "error counts: defaultdict(<class 'int'>, {})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/tf/datadrive/brat-v1.3_Crunchy_Frog/data/examples/rucor_v5_fixed_edges/\"\n",
    "\n",
    "examples = parse_collection(\n",
    "    data_dir=data_dir,\n",
    "    n=None,\n",
    "    ner_encoding=NerEncodings.BIO,\n",
    "    ner_prefix_joiner=NerPrefixJoiners.HYPHEN,\n",
    "    tokens_pattern=None,\n",
    "    allow_nested_entities=True,\n",
    "    allow_nested_entities_one_label=True,\n",
    "    allow_many_entities_per_span_one_label=True,\n",
    "    allow_many_entities_per_span_different_labels=False,\n",
    "    ignore_bad_examples=True,\n",
    "    read_fn=read_file_v3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[163] head T408092 <bos>Младший мой<eos> has already dep T408091 <bos>Витька<eos>, but tried to assign dep T408094 <bos>сам<eos>\n",
      "[165] head T409624 <bos>Я<eos> has already dep T409622 <bos>Мальчик<eos>, but tried to assign dep T409626 <bos>мне<eos>\n",
      "[169] head T409784 <bos>мальчиком<eos> has already dep T409782 <bos>его<eos>, but tried to assign dep T409786 <bos>своим мальчиком<eos>\n",
      "[175] head T409508 <bos>она<eos> has already dep T409507 <bos>ей<eos>, but tried to assign dep T409511 <bos>она<eos>\n",
      "[179] head T415608 <bos>Солнца<eos> has already dep T415577 <bos>Солнцу<eos>, but tried to assign dep T415610 <bos>Солнца<eos>\n",
      "[181] head T410545 <bos>ее<eos> has already dep T410541 <bos>эта дама<eos>, but tried to assign dep T410549 <bos>ее<eos>\n"
     ]
    }
   ],
   "source": [
    "examples_filtered = []\n",
    "for x in examples:\n",
    "    check_tokens(x)\n",
    "    try:\n",
    "        check_arcs(x, one_child=True, one_parent=False)\n",
    "        examples_filtered.append(x)\n",
    "    except AssertionError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(examples_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **simplify**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_simplified = list(map(simplify, examples_filtered))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **filter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_simplified_filtered = []\n",
    "for x in examples_simplified:\n",
    "    is_good = True\n",
    "    try:\n",
    "        check_tokens(x)\n",
    "    except AssertionError as e:\n",
    "        print(\"tokens \" + str(e))\n",
    "        is_good = False\n",
    "    try:\n",
    "        check_arcs(x, one_child=True, one_parent=False)\n",
    "    except AssertionError as e:\n",
    "        print(\"arcs \" + str(e))\n",
    "        is_good = False\n",
    "    if len(x.entities) == 0:\n",
    "        is_good = False\n",
    "    if is_good:\n",
    "        examples_simplified_filtered.append(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **assign id_chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import get_connected_components\n",
    "\n",
    "def assign_id_chain(examples):\n",
    "    for x in examples:\n",
    "        id2entity = {}\n",
    "        g = {}\n",
    "        for entity in x.entities:\n",
    "            g[entity.id] = set()\n",
    "            id2entity[entity.id] = entity\n",
    "        for arc in x.arcs:\n",
    "            g[arc.head].add(arc.dep)\n",
    "\n",
    "        components = get_connected_components(g)\n",
    "\n",
    "        for id_chain, comp in enumerate(components):\n",
    "            for id_entity in comp:\n",
    "                id2entity[id_entity].id_chain = id_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assign_id_chain(examples_simplified_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ds_info(examples, part: str = None):\n",
    "    print(\"part:\", part)\n",
    "    print(\"num examples:\", len(examples))\n",
    "    print(\"num entities:\", sum(len(x.entities) for x in examples))\n",
    "    print(\"num relations:\", sum(len(x.arcs) for x in examples))\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part: None\n",
      "num examples: 174\n",
      "num entities: 14588\n",
      "num relations: 11394\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "ds_info(examples_simplified_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **train / test / valid split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_train, examples_valid, examples_test = train_test_valid_split(\n",
    "    examples=examples_simplified_filtered,\n",
    "    train_frac=0.8,\n",
    "    test_frac=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part: train\n",
      "num examples: 139\n",
      "num entities: 11778\n",
      "num relations: 9203\n",
      "==================================================\n",
      "part: valid\n",
      "num examples: 18\n",
      "num entities: 1343\n",
      "num relations: 1053\n",
      "==================================================\n",
      "part: test\n",
      "num examples: 17\n",
      "num entities: 1467\n",
      "num relations: 1138\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "ds_info(examples_train, \"train\")\n",
    "ds_info(examples_valid, \"valid\")\n",
    "ds_info(examples_test, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **split by chunks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fix_pointers = True\n",
    "\n",
    "def split(examples, window: int, stride: int = 1, ignore_chunks_wo_entities: bool = True):\n",
    "    chunks = []\n",
    "    for x in examples:\n",
    "        chunks_i = split_example_v2(\n",
    "            x, \n",
    "            window=window, \n",
    "            stride=stride, \n",
    "            lang=Languages.RU, \n",
    "            tokens_expression=None,\n",
    "            fix_pointers=fix_pointers\n",
    "        )\n",
    "        for chunk in chunks_i:\n",
    "            if len(chunk.entities) > 0:\n",
    "                x.chunks.append(chunk)\n",
    "            else:\n",
    "                if not ignore_chunks_wo_entities:\n",
    "                    x.chunks.append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0513 07:51:01.509605 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:01.531503 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:01.556564 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:01.569420 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:02.001575 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:02.011615 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:02.025994 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:02.061110 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:02.076445 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:02.105899 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:02.141688 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:02.153263 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:02.192290 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:02.211089 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:02.443361 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:02.985794 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:03.023729 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:03.173436 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:03.184760 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:03.194023 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:03.317848 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:03.333382 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:03.351541 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:03.395144 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:03.414415 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:03.448094 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:03.463692 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:03.489075 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:03.539317 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:04.161031 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:04.229078 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:04.244023 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:04.287888 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:04.298914 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:04.385492 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:04.405230 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:04.417033 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:04.437396 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:04.459042 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:04.522418 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:04.578443 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:04.599107 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:04.624080 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:04.660869 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:04.708679 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:04.747455 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:04.771425 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:05.587592 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:05.604266 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:05.612017 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:05.623087 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:05.687330 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:05.762092 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:05.765030 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:05.774632 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:05.907312 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:06.116964 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:06.151147 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:06.255615 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:06.311776 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:06.362473 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:06.431284 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:06.447731 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:07.282063 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:07.396969 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:07.426460 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:07.514833 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:07.581408 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:07.623455 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:07.637950 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:07.647339 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:07.658652 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:07.674966 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:07.752522 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:07.782543 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:07.808807 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:07.842410 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:07.856495 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:07.892254 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:07.917887 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:07.926059 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:07.985424 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:08.026279 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:08.065292 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:08.157475 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:08.178076 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:08.332994 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:08.384477 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:09.403471 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:09.445685 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:09.501570 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:09.541699 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:09.583224 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:09.646855 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:09.671673 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:09.685465 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:09.811964 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:09.832078 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:09.864465 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:10.021630 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:10.070727 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:10.085316 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:10.114413 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:10.237194 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:10.267391 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:10.315231 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:10.396997 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:10.435547 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:10.468862 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:10.477367 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:10.511183 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:10.557025 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:10.575749 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:10.644115 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:10.679263 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:10.747416 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:10.773499 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:10.804597 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:10.820526 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:12.066572 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:12.100156 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:12.138947 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:12.236665 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:12.324949 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:12.372591 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:12.391863 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:12.454483 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:12.492359 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:12.527230 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:12.562901 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:12.591168 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:12.639479 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:12.672847 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n",
      "W0513 07:51:12.689147 139799476565824 tokenizer.py:121] Something went wrong while tokenizing\n"
     ]
    }
   ],
   "source": [
    "window = 5\n",
    "stride = 1\n",
    "ignore_chunks_wo_entities = True\n",
    "\n",
    "split(examples_train + examples_valid + examples_test, window, stride, ignore_chunks_wo_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = []\n",
    "for x in examples_train + examples_valid + examples_test:\n",
    "    chunks += x.chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in chunks:\n",
    "    try:\n",
    "        check_tokens(x)\n",
    "    except AssertionError as e:\n",
    "        print(\"tokens \" + str(e))\n",
    "    try:\n",
    "        check_arcs(x, one_child=True, one_parent=False)\n",
    "    except AssertionError as e:\n",
    "        print(\"arcs \" + str(e))\n",
    "    try:\n",
    "        check_split(x, window=window, fixed_sent_pointers=fix_pointers)\n",
    "    except AssertionError as e:\n",
    "        print(\"split \" + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in chunks_train + chunks_valid + chunks_test:\n",
    "#     check_example(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part: None\n",
      "num examples: 6479\n",
      "num entities: 64973\n",
      "num relations: 35950\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "ds_info(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **bpe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tf/datadrive/nn_lfs/experiments/bert_for_cr_mention_ranking_10_epochs_fixed/vocab.txt'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_dir = \"/tf/datadrive/nn_lfs/pretrained/rubert_cased_L-12_H-768_A-12_v2/\"\n",
    "vocab_file = os.path.join(bert_dir, \"vocab.txt\")\n",
    "shutil.copy(vocab_file, os.path.join(MODEL_DIR, \"vocab.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0513 07:51:13.111860 139799476565824 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = FullTokenizer(vocab_file=vocab_file, do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in chunks:\n",
    "    apply_bpe(\n",
    "        x, \n",
    "        tokenizer=tokenizer, \n",
    "        ner_prefix_joiner=NerPrefixJoiners.HYPHEN,\n",
    "        ner_encoding=NerEncodings.BIO\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for x in examples_train + examples_valid + examples_test:\n",
    "    x.chunks = [chunk for chunk in x.chunks if all(len(t.token_ids) > 0 for t in chunk.tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6479, 6473)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks), sum(len(x.chunks) for x in examples_train + examples_valid + examples_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **enumerate entities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chunks:\n",
    "    enumerate_entities(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Конфигурация модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config = json.load(open(os.path.join(bert_dir, \"bert_config.json\")))\n",
    "bert_config[\"attention_probs_dropout_prob\"] = 0.5  # default 0.1\n",
    "bert_config[\"hidden_dropout_prob\"] = 0.1  # default 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"model\": {\n",
    "        \"birnn\": {\n",
    "            \"use\": False,\n",
    "            \"params\": {}\n",
    "        },\n",
    "        \"bert\": {\n",
    "            \"test_mode\": False,\n",
    "            \"dropout\": 0.2,\n",
    "            \"scope\": \"bert\",\n",
    "            \"pad_token_id\": tokenizer.vocab[\"[PAD]\"],\n",
    "            \"cls_token_id\": tokenizer.vocab[\"[CLS]\"],\n",
    "            \"sep_token_id\": tokenizer.vocab[\"[SEP]\"],\n",
    "            \"params\": bert_config\n",
    "        },\n",
    "        \"coref\": {\n",
    "            \"use_birnn\": False,\n",
    "            \"rnn\": {\n",
    "                \"num_layers\": 1,\n",
    "                \"cell_dim\": 256,\n",
    "                \"dropout\": 0.5,\n",
    "                \"recurrent_dropout\": 0.0\n",
    "            },\n",
    "            \"use_attn\": True,\n",
    "            \"attn\": {\n",
    "                \"hidden_dim\": 128,\n",
    "                \"dropout\": 0.3,\n",
    "                \"activation\": \"relu\"\n",
    "            },\n",
    "            \"hoi\": {\n",
    "                \"order\": 1,\n",
    "                \"w_dropout\": 0.5,\n",
    "                \"w_dropout_policy\": 0  # 0 - one mask; 1 - different mask\n",
    "            },\n",
    "            \"biaffine\": {\n",
    "                \"num_mlp_layers\": 1,\n",
    "                \"activation\": \"relu\",\n",
    "                \"head_dim\": 128,\n",
    "                \"dep_dim\": 128,\n",
    "                \"dropout\": 0.33,\n",
    "                \"num_labels\": 1,\n",
    "                \"use_dep_prior\": False\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"init_lr\": 2e-5,\n",
    "        \"warmup_proportion\": 0.1\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"num_epochs\": 10,\n",
    "        \"batch_size\": 8,  # уменьшен, т.к. maxlen=256\n",
    "        \"max_epochs_wo_improvement\": 20,\n",
    "        \"num_train_samples\": sum(len(x.chunks) for x in examples_train),\n",
    "        \"maxlen\": 256\n",
    "    },\n",
    "    \"valid\": {\n",
    "        \"path_true\": \"/tmp/gold.conll\",\n",
    "        \"path_pred\": \"/tmp/pred.conll\",\n",
    "        \"scorer_path\": \"/tf/datadrive/reference-coreference-scorers/scorer.pl\",\n",
    "        \"window\": window\n",
    "    },\n",
    "    \"inference\": {\n",
    "        \"max_tokens_per_batch\": 10000,\n",
    "        \"window\": window,\n",
    "        \"maxlen\": 256\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = get_session()\n",
    "model = BertForCoreferenceResolutionMentionRanking(sess=sess, config=config)\n",
    "model.build(mode=ModeKeys.TRAIN)\n",
    "model.save_config(model_dir=MODEL_DIR)\n",
    "model.reset_weights(bert_dir=bert_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num bert tvars: 177853440\n",
      "num non bert tvars: 707458\n"
     ]
    }
   ],
   "source": [
    "def tvars_info():\n",
    "    num_bert_tvars = 0\n",
    "    num_non_bert_tvars = 0\n",
    "    for v in tf.trainable_variables():\n",
    "        n = 1\n",
    "        for d in v.shape:\n",
    "            n *= d\n",
    "        if v.name.startswith(\"model/bert\"):\n",
    "            num_bert_tvars += n\n",
    "        else:\n",
    "            num_non_bert_tvars += n\n",
    "    print(\"num bert tvars:\", num_bert_tvars)\n",
    "    print(\"num non bert tvars:\", num_non_bert_tvars)\n",
    "\n",
    "tvars_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from tensorflow.python.pywrap_tensorflow import NewCheckpointReader\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def check_initialization():\n",
    "#     checkpoint_path = os.path.join(bert_dir, \"bert_model.ckpt\")\n",
    "#     reader = NewCheckpointReader(checkpoint_path)\n",
    "\n",
    "#     for v in tqdm(tf.trainable_variables()):\n",
    "#         name = model._actual_name_to_checkpoint_name(v.name)\n",
    "#         if name.startswith(\"bert\"):\n",
    "#             ckpt_value = reader.get_tensor(name)\n",
    "#             init_value = v.eval(session=sess)\n",
    "#             assert np.allclose(ckpt_value, init_value), name\n",
    "\n",
    "# check_initialization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verbose_fn(d):\n",
    "    print(\"loss:\", d[\"loss\"])\n",
    "    print(\"score:\", d[\"score\"])\n",
    "    print(\"num_entities:\", d[\"num_entities\"])\n",
    "    print(\"num_chains_true:\", d[\"num_chains_true\"])\n",
    "    print(\"num_chains_pred:\", d[\"num_chains_pred\"])\n",
    "    print(classification_report_to_string(d[\"metrics\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train started.\n",
      "model dir /tf/datadrive/nn_lfs/experiments/bert_for_cr_mention_ranking_10_epochs_fixed exists\n",
      "checkpoint path: /tf/datadrive/nn_lfs/experiments/bert_for_cr_mention_ranking_10_epochs_fixed/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/634 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of ignored examples: 76\n",
      "following examples are ignored due to their length is > 256 pieces:\n",
      "105_6-11 => 261\n",
      "178_64-69 => 281\n",
      "178_65-70 => 289\n",
      "178_66-71 => 299\n",
      "178_67-72 => 307\n",
      "1_9-14 => 267\n",
      "1_10-15 => 302\n",
      "1_11-16 => 384\n",
      "1_12-17 => 291\n",
      "95_14-19 => 283\n",
      "95_15-20 => 303\n",
      "95_16-21 => 296\n",
      "95_24-29 => 300\n",
      "95_25-30 => 269\n",
      "95_26-31 => 263\n",
      "95_27-32 => 273\n",
      "76_69-74 => 292\n",
      "76_70-75 => 281\n",
      "76_71-76 => 353\n",
      "76_72-77 => 324\n",
      "76_73-78 => 295\n",
      "118_22-27 => 305\n",
      "118_23-28 => 313\n",
      "118_24-29 => 315\n",
      "118_25-30 => 351\n",
      "118_26-31 => 264\n",
      "118_125-130 => 272\n",
      "118_126-131 => 341\n",
      "118_127-132 => 332\n",
      "118_128-133 => 329\n",
      "118_129-134 => 342\n",
      "97_18-23 => 307\n",
      "97_19-24 => 303\n",
      "97_20-25 => 347\n",
      "97_21-26 => 304\n",
      "97_22-27 => 263\n",
      "97_31-36 => 266\n",
      "97_32-37 => 264\n",
      "60_20-25 => 314\n",
      "60_21-26 => 339\n",
      "6_27-32 => 300\n",
      "6_28-33 => 400\n",
      "6_29-34 => 348\n",
      "6_30-35 => 327\n",
      "6_32-37 => 262\n",
      "106_8-13 => 290\n",
      "106_9-14 => 302\n",
      "106_10-15 => 354\n",
      "106_11-16 => 339\n",
      "106_12-17 => 387\n",
      "106_13-18 => 290\n",
      "106_14-19 => 302\n",
      "106_15-20 => 275\n",
      "106_16-21 => 283\n",
      "106_18-23 => 257\n",
      "106_28-33 => 260\n",
      "106_29-34 => 267\n",
      "106_30-35 => 295\n",
      "106_31-36 => 259\n",
      "106_32-37 => 341\n",
      "106_33-38 => 283\n",
      "106_34-39 => 269\n",
      "64_6-11 => 271\n",
      "64_7-12 => 285\n",
      "64_9-14 => 378\n",
      "64_10-15 => 383\n",
      "64_11-16 => 294\n",
      "64_12-17 => 310\n",
      "64_13-18 => 292\n",
      "51_8-13 => 273\n",
      "51_10-15 => 263\n",
      "167_149-154 => 298\n",
      "167_150-155 => 316\n",
      "167_151-156 => 413\n",
      "167_152-157 => 430\n",
      "167_153-158 => 412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 634/634 [02:48<00:00,  3.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 finished. mean train loss: 1.2191115617752075. evaluation starts.\n",
      "evaluate started.\n",
      "evaluate finished. Time elapsed: 0:00:15.150949\n",
      "loss: 0.5928544139289474\n",
      "score: 0.5962000000000001\n",
      "num_entities: 1343\n",
      "num_chains_true: 290\n",
      "num_chains_pred: 563\n",
      "       f1         precision  recall     \n",
      "\n",
      "bcub   0.6389     0.8197     0.5234     \n",
      "blanc  0.6838     0.7797     0.6498     \n",
      "ceafe  0.47       0.356      0.6912     \n",
      "ceafm  0.5547     0.5547     0.5547     \n",
      "muc    0.7212     0.8474     0.6277\n",
      "current score: 0.5962000000000001\n",
      "!!! new best score: 0.5962000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/634 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new head to /tf/datadrive/nn_lfs/experiments/bert_for_cr_mention_ranking_10_epochs_fixed/model.ckpt\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 634/634 [02:38<00:00,  4.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 finished. mean train loss: 0.8799513578414917. evaluation starts.\n",
      "evaluate started.\n",
      "evaluate finished. Time elapsed: 0:00:11.015968\n",
      "loss: 0.4530867919514702\n",
      "score: 0.6633\n",
      "num_entities: 1343\n",
      "num_chains_true: 290\n",
      "num_chains_pred: 488\n",
      "       f1         precision  recall     \n",
      "\n",
      "bcub   0.6849     0.8455     0.5755     \n",
      "blanc  0.7116     0.8327     0.6697     \n",
      "ceafe  0.5677     0.4526     0.7616     \n",
      "ceafm  0.6113     0.6113     0.6113     \n",
      "muc    0.7893     0.8807     0.715\n",
      "current score: 0.6633\n",
      "!!! new best score: 0.6633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/634 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new head to /tf/datadrive/nn_lfs/experiments/bert_for_cr_mention_ranking_10_epochs_fixed/model.ckpt\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 634/634 [02:38<00:00,  4.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 finished. mean train loss: 0.7059475779533386. evaluation starts.\n",
      "evaluate started.\n",
      "evaluate finished. Time elapsed: 0:00:10.934426\n",
      "loss: 0.4330035331171302\n",
      "score: 0.69725\n",
      "num_entities: 1343\n",
      "num_chains_true: 290\n",
      "num_chains_pred: 445\n",
      "       f1         precision  recall     \n",
      "\n",
      "bcub   0.7063     0.8338     0.6127     \n",
      "blanc  0.7253     0.8273     0.6843     \n",
      "ceafe  0.6156     0.5084     0.7802     \n",
      "ceafm  0.6522     0.6522     0.6522     \n",
      "muc    0.8149     0.8853     0.7549\n",
      "current score: 0.69725\n",
      "!!! new best score: 0.69725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/634 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new head to /tf/datadrive/nn_lfs/experiments/bert_for_cr_mention_ranking_10_epochs_fixed/model.ckpt\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 634/634 [02:40<00:00,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 finished. mean train loss: 0.5945993661880493. evaluation starts.\n",
      "evaluate started.\n",
      "evaluate finished. Time elapsed: 0:00:10.976333\n",
      "loss: 0.43833962530195913\n",
      "score: 0.7011250000000001\n",
      "num_entities: 1343\n",
      "num_chains_true: 290\n",
      "num_chains_pred: 450\n",
      "       f1         precision  recall     \n",
      "\n",
      "bcub   0.7107     0.8447     0.6134     \n",
      "blanc  0.7189     0.8273     0.6778     \n",
      "ceafe  0.62       0.5097     0.791      \n",
      "ceafm  0.6537     0.6537     0.6537     \n",
      "muc    0.8201     0.8936     0.7578\n",
      "current score: 0.7011250000000001\n",
      "!!! new best score: 0.7011250000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/634 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new head to /tf/datadrive/nn_lfs/experiments/bert_for_cr_mention_ranking_10_epochs_fixed/model.ckpt\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 634/634 [02:38<00:00,  4.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 finished. mean train loss: 0.5150562524795532. evaluation starts.\n",
      "evaluate started.\n",
      "evaluate finished. Time elapsed: 0:00:13.492120\n",
      "loss: 0.4695271395937135\n",
      "score: 0.7056749999999999\n",
      "num_entities: 1343\n",
      "num_chains_true: 290\n",
      "num_chains_pred: 447\n",
      "       f1         precision  recall     \n",
      "\n",
      "bcub   0.7153     0.8419     0.6217     \n",
      "blanc  0.7273     0.8269     0.6865     \n",
      "ceafe  0.6216     0.5124     0.7898     \n",
      "ceafm  0.6649     0.6649     0.6649     \n",
      "muc    0.8209     0.8928     0.7597\n",
      "current score: 0.7056749999999999\n",
      "!!! new best score: 0.7056749999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/634 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new head to /tf/datadrive/nn_lfs/experiments/bert_for_cr_mention_ranking_10_epochs_fixed/model.ckpt\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 634/634 [02:39<00:00,  3.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 finished. mean train loss: 0.4564584493637085. evaluation starts.\n",
      "evaluate started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/634 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate finished. Time elapsed: 0:00:10.944138\n",
      "loss: 0.5254830836931652\n",
      "score: 0.700375\n",
      "num_entities: 1343\n",
      "num_chains_true: 290\n",
      "num_chains_pred: 451\n",
      "       f1         precision  recall     \n",
      "\n",
      "bcub   0.7091     0.8429     0.612      \n",
      "blanc  0.7136     0.821      0.6734     \n",
      "ceafe  0.6167     0.5066     0.7879     \n",
      "ceafm  0.6552     0.6552     0.6552     \n",
      "muc    0.8205     0.8946     0.7578\n",
      "current score: 0.700375\n",
      "best score: 0.7056749999999999\n",
      "steps wo improvement: 1\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 634/634 [02:39<00:00,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 finished. mean train loss: 0.4101805090904236. evaluation starts.\n",
      "evaluate started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/634 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate finished. Time elapsed: 0:00:11.021085\n",
      "loss: 0.5323187667103589\n",
      "score: 0.6976\n",
      "num_entities: 1343\n",
      "num_chains_true: 290\n",
      "num_chains_pred: 450\n",
      "       f1         precision  recall     \n",
      "\n",
      "bcub   0.7082     0.8436     0.6102     \n",
      "blanc  0.7096     0.8266     0.6686     \n",
      "ceafe  0.6114     0.5027     0.7801     \n",
      "ceafm  0.6507     0.6507     0.6507     \n",
      "muc    0.8201     0.8936     0.7578\n",
      "current score: 0.6976\n",
      "best score: 0.7056749999999999\n",
      "steps wo improvement: 2\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 634/634 [02:37<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 finished. mean train loss: 0.37285664677619934. evaluation starts.\n",
      "evaluate started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/634 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate finished. Time elapsed: 0:00:13.551070\n",
      "loss: 0.575683145860261\n",
      "score: 0.7036250000000001\n",
      "num_entities: 1343\n",
      "num_chains_true: 290\n",
      "num_chains_pred: 436\n",
      "       f1         precision  recall     \n",
      "\n",
      "bcub   0.7067     0.838      0.611      \n",
      "blanc  0.7141     0.8277     0.6729     \n",
      "ceafe  0.629      0.5236     0.7873     \n",
      "ceafm  0.6574     0.6574     0.6574     \n",
      "muc    0.8214     0.8875     0.7644\n",
      "current score: 0.7036250000000001\n",
      "best score: 0.7056749999999999\n",
      "steps wo improvement: 3\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 634/634 [02:39<00:00,  3.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 finished. mean train loss: 0.3420267105102539. evaluation starts.\n",
      "evaluate started.\n",
      "evaluate finished. Time elapsed: 0:00:10.942943\n",
      "loss: 0.6106913581222116\n",
      "score: 0.709025\n",
      "num_entities: 1343\n",
      "num_chains_true: 290\n",
      "num_chains_pred: 423\n",
      "       f1         precision  recall     \n",
      "\n",
      "bcub   0.7178     0.8221     0.637      \n",
      "blanc  0.7334     0.8117     0.6964     \n",
      "ceafe  0.6247     0.5265     0.7679     \n",
      "ceafm  0.6634     0.6634     0.6634     \n",
      "muc    0.8302     0.8902     0.7777\n",
      "current score: 0.709025\n",
      "!!! new best score: 0.709025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/634 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new head to /tf/datadrive/nn_lfs/experiments/bert_for_cr_mention_ranking_10_epochs_fixed/model.ckpt\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 634/634 [02:40<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 finished. mean train loss: 0.3170735836029053. evaluation starts.\n",
      "evaluate started.\n",
      "evaluate finished. Time elapsed: 0:00:10.936375\n",
      "loss: 0.6164409543292534\n",
      "score: 0.70235\n",
      "num_entities: 1343\n",
      "num_chains_true: 290\n",
      "num_chains_pred: 435\n",
      "       f1         precision  recall     \n",
      "\n",
      "bcub   0.706      0.8337     0.6123     \n",
      "blanc  0.7154     0.8218     0.6751     \n",
      "ceafe  0.6227     0.5189     0.7784     \n",
      "ceafm  0.6567     0.6567     0.6567     \n",
      "muc    0.824      0.8898     0.7673\n",
      "current score: 0.70235\n",
      "best score: 0.709025\n",
      "steps wo improvement: 1\n",
      "==================================================\n",
      "restoring model from /tf/datadrive/nn_lfs/experiments/bert_for_cr_mention_ranking_10_epochs_fixed/model.ckpt\n",
      "train finished. Time elapsed: 0:28:51.959571\n"
     ]
    }
   ],
   "source": [
    "model.train(\n",
    "    examples_train=examples_train,\n",
    "    examples_valid=examples_valid,\n",
    "    model_dir=MODEL_DIR,\n",
    "    verbose=True,\n",
    "    verbose_fn=verbose_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate started.\n",
      "evaluate finished. Time elapsed: 0:00:11.998621\n",
      "loss: 0.6534284791141826\n",
      "score: 0.65395\n",
      "num_entities: 1467\n",
      "num_chains_true: 329\n",
      "num_chains_pred: 566\n",
      "       f1         precision  recall     \n",
      "\n",
      "bcub   0.6748     0.8216     0.5725     \n",
      "blanc  0.6977     0.7889     0.6559     \n",
      "ceafe  0.5583     0.4414     0.7594     \n",
      "ceafm  0.6128     0.6128     0.6128     \n",
      "muc    0.7699     0.8712     0.6898\n"
     ]
    }
   ],
   "source": [
    "d_test = model.evaluate(examples=examples_test)\n",
    "verbose_fn(d_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tf.reset_default_graph()\n",
    "# sess = get_session()\n",
    "# model = BertForCoreferenceResolutionMentionRanking.load(sess=sess, model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "examples_test_copy = copy.deepcopy(examples_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in examples_test_copy:\n",
    "    x.arcs = []\n",
    "    for entity in x.entities:\n",
    "        entity.id_chain = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict started.\n",
      "predict finished. Time elapsed: 0:00:07.626766\n"
     ]
    }
   ],
   "source": [
    "model.predict(examples=examples_test_copy, flat_chains=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.io import to_brat_v2\n",
    "\n",
    "output_dir = \"/tf/datadrive/brat-v1.3_Crunchy_Frog/data/examples/rucor_v5_preds_80_10_10_mention_ranking_10_epochs_flat_chains/\"\n",
    "to_brat_v2(examples=examples_test_copy, output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
